---
title: Everything I know about Recurrent Neural Networks
layout: post
---

Recurrent Neural Networks (RNNs) can be used to model sequential data, such as genomic data or stock trends. By modelling the sequence we can do many different things for example we might want to be able to predict the next element in the sequence, or maybe we would like to translate a piece of text. 
A traditional Neural Network can only take in a fixed length (the number of input nodes) A RNN does away with this by allowing us to take in a variable length input and model it.
We would like to model long-term dependencies in data, so we'd also need to be able to integrate all the data from the sequence and us it as an input to model the rest of the sequence
To properly model a sequence we need to (from [this lecture](https://www.youtube.com/watch?v=_h66BW-xNgk)):
1. Handle __variable-length__ sequences
2. Track __long-term__ dependencies
3. Maintain information about __order__
4. __Share parameters__ across the sequence

RNNs are a good answer to the above demands
We can have many different RNN architectures, most interestingly we can also create RNNs that output a _sequence_ instead of just a single number or classification. 
### How is a RNN different from a Vanilla NN
the recurrence relation at every time step is below. we take a function parameterized by %%W%% (weights) that take in as inputs the old state %%h_{t-1}%% and the input vector at timestep t as %%x_t%%

$$h_t = f_w(h_{t-1},x_t)$$

The function to update the hidden state is given as follows, we have two weight matrices %%W_{hh}%% and %%W_{xh}%% which are each applied to the previous state and the input respectively, next we apply a nonlinearity (tanh in this case) to the sum of these vectors which gives us a new %%h_t%% 

$$h_t = tanh(W_{hh}h_{t-1} + W_{xh}x_t)$$

The output is then found using a different weight matrix %%W_{hy}%% as and visualized in the image (also from the lecture)

$$\hat{y_t} = W_{hy}h_t$$ 

![Unrolled RNN](/assets/RNN_unrolled.png)

## LSTM
## GRU